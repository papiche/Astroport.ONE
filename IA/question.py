#!/usr/bin/env python3
import ollama
import argparse
import json

def get_ollama_answer(prompt, model_name="qwen2.5"):
    """
    Generates an answer from Ollama based on the given prompt.

    Args:
        prompt (str): The prompt to send to Ollama.
        model_name (str, optional): The name of the Ollama model to use. Defaults to "qwen2.5".

    Returns:
        str: The answer generated by Ollama.
             Returns None if an error occurs.
    """
    try:
        ai_response = ollama.chat(
            model=model_name,
            messages=[
                {
                    'role': 'user',
                    'content': prompt,
                },
            ]
        )
        answer = ai_response['message']['content']
        return answer
    except ollama.exceptions.OllamaError as e:
        print(f"Error during Ollama processing in question.py: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred in question.py: {e}")
        return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Answer a question using Ollama.")
    parser.add_argument("question", help="The question to ask Ollama.")
    parser.add_argument("-m", "--model", dest="ollama_model_name", default="qwen2.5", help="The name of the Ollama model to use (default: qwen2.5).")
    parser.add_argument("--json", action="store_true", help="Output answer in JSON format.")

    args = parser.parse_args()

    answer_output = get_ollama_answer(args.question, args.ollama_model_name)

    if answer_output:
        if args.json:
            result = {"answer": answer_output}
            print(json.dumps(result))
        else:
            print(answer_output)
    else:
        if not args.json:
            print("Failed to get answer from Ollama.")
