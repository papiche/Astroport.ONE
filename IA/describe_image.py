#!/usr/bin/env python3
import os
import sys

# Activate ~/.astro virtual environment to access ollama module
venv_path = os.path.expanduser("~/.astro")
if os.path.exists(venv_path):
    # Add venv site-packages to sys.path
    python_version = f"python{sys.version_info.major}.{sys.version_info.minor}"
    site_packages = os.path.join(venv_path, "lib", python_version, "site-packages")
    if os.path.exists(site_packages):
        sys.path.insert(0, site_packages)

import requests
import ollama
import tempfile
import argparse
import json
import subprocess
import socket

def check_ollama_port(port=11434):
    """
    Check if the Ollama port is open locally.
    
    Args:
        port (int): The port to check (default: 11434)
    
    Returns:
        bool: True if port is open, False otherwise
    """
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(1)
        result = sock.connect_ex(('127.0.0.1', port))
        sock.close()
        return result == 0
    except Exception:
        return False

def ensure_ollama_connection(output_json=False):
    """
    Ensure that Ollama connection is available by checking the port
    and calling ollama.me.sh if needed.
    
    Args:
        output_json (bool): Whether we're in JSON output mode (to suppress debug prints)
    
    Returns:
        bool: True if connection is established, False otherwise
    """
    if check_ollama_port():
        if not output_json:
            print("Ollama port 11434 is already open.")
        return True
    
    if not output_json:
        print("Ollama port not open. Attempting to establish tunnel via ollama.me.sh...")
    
    # Get the directory where this script is located
    script_dir = os.path.dirname(os.path.abspath(__file__))
    ollama_script = os.path.join(script_dir, "ollama.me.sh")
    
    if not os.path.exists(ollama_script):
        if not output_json:
            print(f"Warning: ollama.me.sh not found at {ollama_script}")
        return False
    
    try:
        result = subprocess.run([ollama_script], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            if not output_json:
                print("Ollama tunnel established successfully.")
            return True
        else:
            if not output_json:
                print(f"Failed to establish Ollama tunnel: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        if not output_json:
            print("Timeout while establishing Ollama tunnel.")
        return False
    except Exception as e:
        if not output_json:
            print(f"Error while calling ollama.me.sh: {e}")
        return False

def describe_image_from_ipfs(image_source, ollama_model="minicpm-v", output_json=False, custom_prompt=None):
    """
    Describes an image using Ollama from either an IPFS URL or a local file path.

    Args:
        image_source (str): Either an IPFS URL (http://...) or a local file path.
        ollama_model (str, optional): The name of the Ollama model to use. Defaults to "minicpm-v".
        output_json (bool, optional): Whether to output the description in JSON format. Defaults to False.
        custom_prompt (str, optional): Custom prompt to use instead of default. Defaults to None.

    Returns:
        str or dict: The description of the image generated by Ollama.
                     If output_json is True, returns a JSON string.
                     Otherwise, returns a plain text string.
                     Returns None if an error occurs.
    """
    # Ensure Ollama connection is available
    if not ensure_ollama_connection(output_json):
        if not output_json:
            print("Warning: Could not establish Ollama connection. Attempting anyway...")
    
    temp_image_path = None
    temp_file_created = False
    
    try:
        # Determine if image_source is a URL or a local file
        if image_source.startswith('http://') or image_source.startswith('https://'):
            # Download from IPFS URL
            if not output_json:
                print(f"Downloading image from IPFS URL: {image_source}")
            response = requests.get(image_source, stream=True, timeout=10)
            response.raise_for_status()

            with tempfile.NamedTemporaryFile(delete=False, suffix="") as tmp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    tmp_file.write(chunk)
                temp_image_path = tmp_file.name
                temp_file_created = True

            if not output_json:
                print(f"Image downloaded and saved to temporary file: {temp_image_path}")
        else:
            # Use local file path directly
            if not os.path.exists(image_source):
                if not output_json:
                    print(f"Error: Local file not found: {image_source}")
                return None
            
            temp_image_path = image_source
            temp_file_created = False
            
            if not output_json:
                print(f"Using local image file: {temp_image_path}")

        # Use custom prompt if provided, otherwise use default
        prompt = custom_prompt if custom_prompt else 'Décrire cette image.'
        
        if not output_json:
            print(f"Sending image to Ollama model '{ollama_model}' with prompt: '{prompt[:50]}...'")
        ai_response = ollama.chat(
            model=ollama_model,
            messages=[
                {
                    'role': 'user',
                    'content': prompt,
                    'images': [temp_image_path],
                },
            ]
        )

        description = ai_response['message']['content']
        if not output_json:
            print(f"Ollama description received.")

        if output_json:
            result = {"description": description}
            return json.dumps(result)
        else:
            return description

    except requests.exceptions.RequestException as e:
        if not output_json:
            print(f"Error downloading image from IPFS: {e}")
        return None
    except ConnectionError as e:
        if not output_json:
            print(f"Error connecting to Ollama: {e}")
        return None
    except Exception as e:
        if not output_json:
            print(f"An unexpected error occurred: {e}")
        return None
    finally:
        # Only remove the temp file if we created it
        if temp_file_created and temp_image_path and os.path.exists(temp_image_path):
            os.remove(temp_image_path)
            if not output_json:
                print(f"Temporary image file '{temp_image_path}' removed.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Describe an image from an IPFS URL or local file using Ollama.")
    parser.add_argument("image_source", help="Either an IPFS URL (http://...) or a local file path.")
    parser.add_argument("-m", "--model", dest="ollama_model_name", default="minicpm-v", help="The name of the Ollama model to use (default: minicpm-v).")
    parser.add_argument("--json", action="store_true", help="Output description in JSON format.")
    parser.add_argument("-p", "--prompt", dest="custom_prompt", default=None, help="Custom prompt to send to the AI (default: 'Décrire cette image.').")

    args = parser.parse_args()

    description_output = describe_image_from_ipfs(args.image_source, args.ollama_model_name, args.json, args.custom_prompt)

    if description_output:
        if args.json:
            print(description_output) # Already JSON string
        else:
            print("\nImage Description from Ollama:")
            print(description_output)
    else:
        if not args.json: # Only print error message if not in JSON mode (as JSON mode is for pure output)
            print("\nFailed to get image description.")
